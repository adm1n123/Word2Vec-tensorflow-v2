Skip gram sampling table is based on frequency of words and gives high probability of sampling for least frequent words
so (try to find freq and sort the words then generate indexing for words then pass to sequence for training).


Index 0 in vocab is treated as non-word by skipgrams() and no pairs(+ve/-ve) is generated with 0 as word index.
ie if index 0 appears in sequence it is skipped. from index 1 all the words are sampled.
each target word is chosen with sampling_table[i] probability and then training examples are generated for every
word in context(within window). padding can neither be target nor context. but [UNK] could be both. but
probability of UNK being target is very-very low since it is most frequent word.



General Errors:
    ### this usually occurs when vocabulary is not created(vocab size = 0) and embedding layer creating is called.
    since embedding layers needs vocab size before hand.

    self.target_embedding = Embedding(
      File "C:\Users\4dm1n123\PycharmProjects\Word2Vec\venv\lib\site-packages\tensorflow\python\keras\layers\embeddings.py", line 106, in __init__
        if input_dim <= 0 or output_dim <= 0:
    TypeError: '<=' not supported between instances of 'NoneType' and 'int'